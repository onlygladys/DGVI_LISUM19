{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 6-TASK\n",
    "\n",
    "File Ingestion and Schema Validation:\n",
    "Take any csv/text file of 2+ GB of your choice. --- (You can do this assignment on Google colab)\n",
    "\n",
    "Read the file ( Present approach of reading the file )\n",
    "\n",
    "Try different methods of file reading eg: Dask, Modin, Ray, pandas and present your findings in term of computational efficiency\n",
    "\n",
    "Perform basic validation on data columns : eg: remove special character , white spaces from the col name\n",
    "\n",
    "As you already know the schema hence create a YAML file and write the column name in YAML file. --define separator of read and write file, column name in YAML\n",
    "\n",
    "Validate number of columns and column name of ingested file with YAML.\n",
    "\n",
    "Write the file in pipe separated text file (|) in gz format.\n",
    "\n",
    "Create a summary of the file:\n",
    "\n",
    "Total number of rows,\n",
    "\n",
    "total number of columns\n",
    "\n",
    "file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3031783420"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the size of the raw file:\n",
    "os.path.getsize(r\"D:\\DataGlacier\\Week6\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.824 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path1 = r\"D:\\DataGlacier\\Week6\\test.csv\"\n",
    "\n",
    "def get_size(file_path1, unit='bytes'):\n",
    "    file_size = os.path.getsize(file_path1)\n",
    "    exponents_map = {'bytes': 0, 'kb': 1, 'mb': 2, 'gb': 3}\n",
    "    if unit not in exponents_map:\n",
    "        raise ValueError(\"Must select from \\\n",
    "        ['bytes', 'kb', 'mb', 'gb']\")\n",
    "    else:\n",
    "        size = file_size / 1024 ** exponents_map[unit]\n",
    "        return round(size, 3)\n",
    "\n",
    "\n",
    "print(get_size(file_path1,'gb'),'GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files with various approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "with open(r\"D:\\DataGlacier\\Week6\\test.csv\") as csvfile:  \n",
    "    df = csv.DictReader(csvfile)\n",
    "start_time = time.time()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictReader takes 2.8767387866973877 seconds\n"
     ]
    }
   ],
   "source": [
    "#Read with DictReader\n",
    "\n",
    "star_time=time.time()\n",
    "dfdict = csv.DictReader(open(r\"D:\\DataGlacier\\Week6\\test.csv\"))\n",
    "a=(\"DictReader takes %s seconds\" % (time.time() - start_time))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas takes 162.5535728931427 seconds\n"
     ]
    }
   ],
   "source": [
    "#Read with Pandas\n",
    "\n",
    "import pandas as pd\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "dfpandas = pd.read_csv(open(r\"D:\\DataGlacier\\Week6\\test.csv\"))\n",
    "b=(\"Pandas takes %s seconds\" % (time.time() - start_time))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaskDataframe takes 0.06248784065246582 seconds\n"
     ]
    }
   ],
   "source": [
    "#Read with Dask\n",
    "\n",
    "import dask.dataframe\n",
    "start_time=time.time()\n",
    "dfdask = dask.dataframe.read_csv(r\"D:\\DataGlacier\\Week6\\test.csv\")\n",
    "c=(\"DaskDataframe takes %s seconds\" % (time.time() - start_time))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:34:58,824\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modin and Ray takes 82.9180121421814 seconds\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={'env_vars': {'__MODIN_AUTOIMPORT_PANDAS__': '1'}})\n",
    "start_time=time.time()\n",
    "dfmod=pd.read_csv(r\"D:\\DataGlacier\\Week6\\test.csv\",chunksize=2000000)\n",
    "for chunksize in dfmod:\n",
    "#print(chunksize)\n",
    " d=(\"Modin and Ray takes %s seconds\" % (time.time() - start_time))\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictReader takes 2.8767387866973877 seconds\n",
      "Pandas takes 162.5535728931427 seconds\n",
      "DaskDataframe takes 0.06248784065246582 seconds\n",
      "Modin and Ray takes 82.9180121421814 seconds\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least time taken to read a csv file is:  DaskDataframe takes 0.06248784065246582 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"The least time taken to read a csv file is: \", min(a,b,c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "df= dask.dataframe.read_csv(r\"D:\\DataGlacier\\Week6\\test.csv\",delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 11 entries, Timestamp to Is Laundering\n",
      "dtypes: object(6), float64(2), int64(3)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From Bank</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Is Laundering</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: describe-numeric, 1182 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              From Bank  To Bank Amount Received Amount Paid Is Laundering\n",
       "npartitions=1                                                             \n",
       "                float64  float64         float64     float64       float64\n",
       "                    ...      ...             ...         ...           ...\n",
       "Dask Name: describe-numeric, 1182 tasks"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "df.columns=df.columns.str.replace('[#,@,&,_]','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove white space from columns\n",
    "df.columns = df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'FromBank', 'Account', 'ToBank', 'Account.1',\n",
       "       'AmountReceived', 'ReceivingCurrency', 'AmountPaid', 'PaymentCurrency',\n",
       "       'PaymentFormat', 'IsLaundering'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdfdask=df.columns\n",
    "newdfdask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testutility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile testutility.py\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re\n",
    "\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "\n",
    "def replacer(string, char):\n",
    "    pattern = char + '{2,}'\n",
    "    string = re.sub(pattern, char, string) \n",
    "    return string\n",
    "\n",
    "def col_header_val(df,table_config):\n",
    "    '''\n",
    "    replace whitespaces in the column\n",
    "    and standardized column names\n",
    "    '''\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    expected_col.sort()\n",
    "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile file.yaml\n",
    "file_type: csv\n",
    "dataset_name: testfile\n",
    "file_name: test\n",
    "table_name: edsurv\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns:\n",
    "     -Timestamp\n",
    "     -FromBank\n",
    "     -Account\n",
    "     -ToBank\n",
    "     -Account.1\n",
    "     -AmountReceived\n",
    "     -ReceivingCurrency\n",
    "     -AmountPaid\n",
    "     -PaymentCurrency\n",
    "     -PaymentFormat\n",
    "     -IsLaundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading config file\n",
    "\n",
    "import testutility as util\n",
    "config_data = util.read_config_file('file.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data of config file\n",
    "config_data['inbound_delimiter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_type': 'csv',\n",
       " 'dataset_name': 'testfile',\n",
       " 'file_name': 'test',\n",
       " 'table_name': 'edsurv',\n",
       " 'inbound_delimiter': ',',\n",
       " 'outbound_delimiter': '|',\n",
       " 'skip_leading_rows': 1,\n",
       " 'columns': '-Timestamp -FromBank -Account -ToBank -Account.1 -AmountReceived -ReceivingCurrency -AmountPaid -PaymentCurrency -PaymentFormat -IsLaundering'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>Account</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>Account.1</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/09/01 00:17</td>\n",
       "      <td>20</td>\n",
       "      <td>800104D70</td>\n",
       "      <td>20</td>\n",
       "      <td>800104D70</td>\n",
       "      <td>6794.63</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>6794.63</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>3196</td>\n",
       "      <td>800107150</td>\n",
       "      <td>3196</td>\n",
       "      <td>800107150</td>\n",
       "      <td>7739.29</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>7739.29</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/09/01 00:17</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E430</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E430</td>\n",
       "      <td>1880.23</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>1880.23</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/09/01 00:03</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E650</td>\n",
       "      <td>20</td>\n",
       "      <td>80010E6F0</td>\n",
       "      <td>73966883.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>73966883.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E650</td>\n",
       "      <td>20</td>\n",
       "      <td>80010EA30</td>\n",
       "      <td>45868454.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>45868454.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
       "0  2022/09/01 00:17         20  800104D70       20  800104D70   \n",
       "1  2022/09/01 00:02       3196  800107150     3196  800107150   \n",
       "2  2022/09/01 00:17       1208  80010E430     1208  80010E430   \n",
       "3  2022/09/01 00:03       1208  80010E650       20  80010E6F0   \n",
       "4  2022/09/01 00:02       1208  80010E650       20  80010EA30   \n",
       "\n",
       "   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n",
       "0          6794.63          US Dollar      6794.63        US Dollar   \n",
       "1          7739.29          US Dollar      7739.29        US Dollar   \n",
       "2          1880.23          US Dollar      1880.23        US Dollar   \n",
       "3      73966883.00          US Dollar  73966883.00        US Dollar   \n",
       "4      45868454.00          US Dollar  45868454.00        US Dollar   \n",
       "\n",
       "  Payment Format  Is Laundering  \n",
       "0   Reinvestment              0  \n",
       "1   Reinvestment              0  \n",
       "2   Reinvestment              0  \n",
       "3         Cheque              0  \n",
       "4         Cheque              0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading process of the file using Dask\n",
    "from dask import dataframe as dd\n",
    "df_sample = dd.read_csv(r\"D:\\DataGlacier\\Week6\\test.csv\",delimiter=',')\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " config_data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/DataGlacier/Week6/test.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the file using config file\n",
    "file_type = config_data['file_type']\n",
    "#source_file = \"D:/DataGlacier/Week6/\" + config_data['file_name'] + f'.{file_type}'\n",
    "source_file= \"D:/DataGlacier/Week6/\"+ config_data['file_name'] + f'.{file_type}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>Account</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>Account.1</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/09/01 00:17</td>\n",
       "      <td>20</td>\n",
       "      <td>800104D70</td>\n",
       "      <td>20</td>\n",
       "      <td>800104D70</td>\n",
       "      <td>6794.63</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>6794.63</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>3196</td>\n",
       "      <td>800107150</td>\n",
       "      <td>3196</td>\n",
       "      <td>800107150</td>\n",
       "      <td>7739.29</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>7739.29</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/09/01 00:17</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E430</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E430</td>\n",
       "      <td>1880.23</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>1880.23</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/09/01 00:03</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E650</td>\n",
       "      <td>20</td>\n",
       "      <td>80010E6F0</td>\n",
       "      <td>73966883.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>73966883.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>1208</td>\n",
       "      <td>80010E650</td>\n",
       "      <td>20</td>\n",
       "      <td>80010EA30</td>\n",
       "      <td>45868454.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>45868454.00</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
       "0  2022/09/01 00:17         20  800104D70       20  800104D70   \n",
       "1  2022/09/01 00:02       3196  800107150     3196  800107150   \n",
       "2  2022/09/01 00:17       1208  80010E430     1208  80010E430   \n",
       "3  2022/09/01 00:03       1208  80010E650       20  80010E6F0   \n",
       "4  2022/09/01 00:02       1208  80010E650       20  80010EA30   \n",
       "\n",
       "   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n",
       "0          6794.63          US Dollar      6794.63        US Dollar   \n",
       "1          7739.29          US Dollar      7739.29        US Dollar   \n",
       "2          1880.23          US Dollar      1880.23        US Dollar   \n",
       "3      73966883.00          US Dollar  73966883.00        US Dollar   \n",
       "4      45868454.00          US Dollar  45868454.00        US Dollar   \n",
       "\n",
       "  Payment Format  Is Laundering  \n",
       "0   Reinvestment              0  \n",
       "1   Reinvestment              0  \n",
       "2   Reinvestment              0  \n",
       "3         Cheque              0  \n",
       "4         Cheque              0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(source_file,config_data['inbound_delimiter'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation failed\n",
      "Following File columns are not in the YAML file ['amount_paid', 'receiving_currency', 'from_bank', 'is_laundering', 'payment_format', 'to_bank', 'account_1', 'timestamp', 'amount_received', 'payment_currency', 'account']\n",
      "Following YAML columns are not in the file uploaded ['a', 'c', 'o', 'g', '-', 'b', 't', ' ', 'v', 'u', 'k', '.', 'y', 'm', '1', 'd', 'p', 'r', 's', 'n', 'e', 'l', 'i', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validating the header of the file\n",
    "util.col_header_val(df1,config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns of files are: Index(['timestamp', 'from_bank', 'account', 'to_bank', 'account_1',\n",
      "       'amount_received', 'receiving_currency', 'amount_paid',\n",
      "       'payment_currency', 'payment_format', 'is_laundering'],\n",
      "      dtype='object')\n",
      "columns of YAML are: -Timestamp -FromBank -Account -ToBank -Account.1 -AmountReceived -ReceivingCurrency -AmountPaid -PaymentCurrency -PaymentFormat -IsLaundering\n"
     ]
    }
   ],
   "source": [
    "print(\"columns of files are:\" ,df1.columns)\n",
    "print(\"columns of YAML are:\" ,config_data['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation failed\n",
      "Following File columns are not in the YAML file ['amount_paid', 'receiving_currency', 'from_bank', 'is_laundering', 'payment_format', 'to_bank', 'account_1', 'timestamp', 'amount_received', 'payment_currency', 'account']\n",
      "Following YAML columns are not in the file uploaded ['a', 'c', 'o', 'g', '-', 'b', 't', ' ', 'v', 'u', 'k', '.', 'y', 'm', '1', 'd', 'p', 'r', 's', 'n', 'e', 'l', 'i', 'f']\n",
      "validation failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if util.col_header_val(df1,config_data)==0:\n",
    "    print(\"validation failed\")\n",
    "else:\n",
    "    print(\"col validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\00.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\01.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\02.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\03.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\04.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\05.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\06.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\07.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\08.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\09.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\10.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\11.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\12.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\13.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\14.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\15.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\16.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\17.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\18.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\19.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\20.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\21.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\22.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\23.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\24.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\25.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\26.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\27.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\28.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\29.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\30.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\31.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\32.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\33.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\34.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\35.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\36.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\37.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\38.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\39.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\40.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\41.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\42.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\43.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\44.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\45.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\46.part',\n",
       " 'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz\\\\47.part']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import gzip\n",
    "\n",
    "from dask import dataframe as dd\n",
    "df = dd.read_csv(r'D:\\DataGlacier\\Week6\\test.csv',delimiter=',')\n",
    "\n",
    "# Write csv in gz format in pipe separated text file (|)\n",
    "df.to_csv('test.csv.gz',\n",
    "          sep='|',\n",
    "          header=True,\n",
    "          index=False,\n",
    "          quoting=csv.QUOTE_ALL,\n",
    "          compression='gzip',\n",
    "          quotechar='\"',\n",
    "          doublequote=True,\n",
    "          line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.part\n",
      "00.part\n",
      "01.part\n",
      "02.part\n",
      "03.part\n",
      "04.part\n",
      "05.part\n",
      "06.part\n",
      "07.part\n",
      "08.part\n",
      "09.part\n",
      "10.part\n",
      "11.part\n",
      "12.part\n",
      "13.part\n",
      "14.part\n",
      "15.part\n",
      "16.part\n",
      "17.part\n",
      "18.part\n",
      "19.part\n",
      "20.part\n",
      "21.part\n",
      "22.part\n",
      "23.part\n",
      "24.part\n",
      "25.part\n",
      "26.part\n",
      "27.part\n",
      "28.part\n",
      "29.part\n",
      "30.part\n",
      "31.part\n",
      "32.part\n",
      "33.part\n",
      "34.part\n",
      "35.part\n",
      "36.part\n",
      "37.part\n",
      "38.part\n",
      "39.part\n",
      "40.part\n",
      "41.part\n",
      "42.part\n",
      "43.part\n",
      "44.part\n",
      "45.part\n",
      "46.part\n",
      "47.part\n"
     ]
    }
   ],
   "source": [
    "#number of files in gz format folder\n",
    "import os\n",
    "entries = os.listdir(r'C:/Users/NammaPC/File ingestion and schema validation/test.csv.gz')\n",
    "for entry in entries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the gz format folder\n",
    "a=os.path.getsize(r'C:\\Users\\NammaPC\\File ingestion and schema validation/test.csv.gz')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = r'C:\\Users\\NammaPC\\File ingestion and schema validation/test.csv.gz'\n",
    "\n",
    "def get_size(file_path, unit='bytes'):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    exponents_map = {'bytes': 0, 'kb': 1, 'mb': 2, 'gb': 3}\n",
    "    if unit not in exponents_map:\n",
    "        raise ValueError(\"Must select from \\\n",
    "        ['bytes', 'kb', 'mb', 'gb']\")\n",
    "    else:\n",
    "        size = file_size / 1024 ** exponents_map[unit]\n",
    "        return round(size, 3)\n",
    "\n",
    "\n",
    "sizeoffile=(get_size(file_path,'mb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Total number of rows are: 31898238\n",
      "Total number of columns are : 11\n",
      "Size of the file is 0.016 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\")\n",
    "\n",
    "# Total number of observations:\n",
    "print(\"Total number of rows are:\",len(df))\n",
    "\n",
    "# Total number of features:\n",
    "print(\"Total number of columns are :\",len(df.columns))\n",
    "df.columns\n",
    "\n",
    "print(\"Size of the file is\",sizeoffile,\"GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
